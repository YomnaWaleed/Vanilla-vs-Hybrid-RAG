{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf75414",
   "metadata": {},
   "source": [
    "### 1) Load index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4b893cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, json, os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2f8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe05a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS\n",
    "emb = GoogleGenerativeAIEmbeddings(\n",
    "  model = \"models/embedding-001\", \n",
    "  google_api_key= GEMINI_API_KEY\n",
    ")\n",
    "faiss_store = FAISS.load_local(\"../artifacts/faiss_index\", emb, allow_dangerous_deserialization= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a803e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BM25 corpus\n",
    "with open(\"../artifacts/bm25_corpus.pkl\", \"rb\") as f:\n",
    "  bm25_data = pickle.load(f)\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "bm25 = BM25Okapi(bm25_data[\"corpus_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee48d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load chunks mapping\n",
    "chunks_map = {}\n",
    "with open(\"../artifacts/chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "  for line in f:\n",
    "    chunk = json.loads(line)\n",
    "    chunks_map[chunk[\"chunk_id\"]] = chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6dfcc",
   "metadata": {},
   "source": [
    "### 2) Retrieve Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a535cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e28759cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(q):\n",
    "  return re.findall(r\"\\b\\w+\\b\", q.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e88b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_bm25(query, k = 10):\n",
    "  scores = bm25.get_scores(tokenize(query))\n",
    "  idx = np.argsort(scores)[::-1][:k]\n",
    "  out = []\n",
    "  for i in idx:\n",
    "    out.append({\n",
    "      \"chunk_id\": i, \n",
    "      \"score\": float(scores[i]),\n",
    "      \"source\" : \"bm25\", \n",
    "      \"text\":chunks_map[i][\"text\"], \n",
    "      \"doc_id\": chunks_map[i][\"doc_id\"], \n",
    "      \"page\":chunks_map[i][\"page\"], \n",
    "      })\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7027ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dense(query, k = 10):\n",
    "  docs_scores = faiss_store.similarity_search_with_score(query, k = k)\n",
    "  \n",
    "  out = []\n",
    "  for doc, dist in docs_scores:\n",
    "    cid = doc.metadata[\"chunk_id\"]\n",
    "    out.append({\n",
    "      \"chunk_id\": cid, \n",
    "      \"score\": float(-dist), \n",
    "      \"source\": \"dense\", \n",
    "      \"text\": doc.page_content, \n",
    "      \"doc_id\": chunks_map[cid][\"doc_id\"], \n",
    "      \"page\": doc.metadata.get(\"page\", None)\n",
    "    })\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573ba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_fusion(candidates_lists, top_k = 8 , k_rff = 60):\n",
    "  # candidates_lists: [list_from_bm25, list_from_dense]\n",
    "  score_by_id = defaultdict(float)\n",
    "  seen_rank = defaultdict(dict)  # source -> chunk_id -> rank\n",
    "  \n",
    "  for cand_list in candidates_lists:\n",
    "    for rank, item in enumerate(cand_list):\n",
    "      cid = item[\"chunk_id\"]\n",
    "      score_by_id[cid] += 1.0 / (k_rff + rank +1)\n",
    "  \n",
    "  # retrieve best top_k \n",
    "  ranked = sorted(score_by_id.items(), key = lambda x: x[1], reverse=True)[:top_k]\n",
    "  \n",
    "  merged = []\n",
    "  pool = { (c[\"chunk_id\"], c[\"source\"]): c for lst in candidates_lists for c in lst }\n",
    "  \n",
    "  for cid, _ in ranked:\n",
    "    # find any item which chunk_id = cid \n",
    "    item = None\n",
    "    for lst in candidates_lists:\n",
    "      for c in lst:\n",
    "        if c[\"chunk_id\"] == cid:\n",
    "          item = c\n",
    "          break\n",
    "      if item:\n",
    "        break\n",
    "    merged.append(item)\n",
    "  return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbce46",
   "metadata": {},
   "source": [
    "Reranker (Cross-Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc820fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPSHOP\\anaconda3\\envs\\slrag\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bd441b14b2466fa1a41655706b64f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPSHOP\\anaconda3\\envs\\slrag\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30d979b8fdf43729fa5ca89248c7e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcca80b5dae14a80bd752a32320591f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1072a49f06be47b3ae6ced135c65ee80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25b932bf2e640258a5811bbeb0c821f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e336e5ad120477da1b4670bda743af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_with_crossencoder(query, candidates, top_k=6):\n",
    "    pairs = [(query, c[\"text\"]) for c in candidates]\n",
    "    scores = reranker.predict(pairs)  # bigger = better\n",
    "    order = np.argsort(scores)[::-1][:top_k]\n",
    "    reranked = [candidates[i] for i in order]\n",
    "    return reranked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9136b9",
   "metadata": {},
   "source": [
    "### 3) Generation the response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e509a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e81ae0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "  api_key = GEMINI_API_KEY,\n",
    "  model = \"gemini-2.5-pro\", \n",
    "  temperature=0.2,\n",
    "  max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfc09816",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = (\n",
    "\"You are an assistant answering automotive standards questions (ASPICE/AUTOSAR). \"\n",
    "\"Answer ONLY from the provided context. If the answer is not in the context, say 'Insufficient context'. \"\n",
    "\"After each factual sentence, add citations like [doc:{doc_id}, page:{page}, chunk:{chunk_id}].\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "315619ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_snippets(evidences):\n",
    "  ctx = []\n",
    "  for e in evidences:\n",
    "        ctx.append(\n",
    "            f\"[doc:{e['doc_id']}, page:{e['page']}, chunk:{e['chunk_id']}]\\n{e['text']}\\n\"\n",
    "        )\n",
    "  return \"\\n---\\n\".join(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "531f3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_hybrid(query, k_each=12, fuse_top=12, rerank_top=6):\n",
    "    bm25_hits = retrieve_bm25(query, k=k_each)\n",
    "    dense_hits = retrieve_dense(query, k=k_each)\n",
    "    fused = rrf_fusion([bm25_hits, dense_hits], top_k=fuse_top)\n",
    "    reranked = rerank_with_crossencoder(query, fused, top_k=rerank_top)\n",
    "\n",
    "    context_block = build_context_snippets(reranked)\n",
    "    prompt = (\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Context (use strictly):\\n{context_block}\\n\\n\"\n",
    "        \"Return a concise answer with citations as instructed.\"\n",
    "    )\n",
    "    msgs = [SystemMessage(content=SYSTEM), HumanMessage(content=prompt)]\n",
    "    resp = llm.invoke(msgs)\n",
    "    return {\"answer\": resp.content, \"evidence\": reranked}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a784a",
   "metadata": {},
   "source": [
    "### 4) practice on 5 queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e5e7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"What does ASPICE SYS.2 require?\",\n",
    "    \"What are the work products expected in ASPICE SYS.1?\",\n",
    "    \"In AUTOSAR ECU State Manager, what is the purpose of RUN state?\",\n",
    "    \"How does AUTOSAR ensure a safe transition between ECU states?\",\n",
    "    \"What is the difference between ASPICE SYS.2 and SWE.1 objectives?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcd9ff8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What does ASPICE SYS.2 require?\n",
      "Based on the context provided, ASPICE SYS.2 is the System Requirements Analysis process [doc:Automotive_SPICE_PAM_31_EN, page:12, chunk:72][doc:Automotive_SPICE_PAM_31_EN, page:4, chunk:26][doc:Automotive_SPICE_PAM_31_EN, page:11, chunk:67]. It is part of the System Engineering process group (SYS) [doc:Automotive_SPICE_PAM_31_EN, page:12, chunk:72]. The process includes base practices related to bidirectional traceability (SYS.2 BP6) and consistency (SYS.2 BP7) [doc:Automotive_SPICE_PAM_31_EN, page:123, chunk:701].\n",
      "\n",
      "The provided context does not contain further details on the specific requirements, purpose, or outcomes of the SYS.2 process.\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 47\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 45\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 41\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What are the work products expected in ASPICE SYS.1?\n",
      "Insufficient context.\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 12\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 4.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 10\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 8.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 6\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 16.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 58\n",
      "}\n",
      "].\n",
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 32.0 seconds as it raised ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-2.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "  quota_value: 2\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 42\n",
      "}\n",
      "].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: In AUTOSAR ECU State Manager, what is the purpose of RUN state?\n",
      "The purpose of the RUN state is to have the ECU State Manager implement all its activities while the OS is up and running [doc:AUTOSAR_SWS_ECUStateManager, page:138, chunk:1234]. RUN is a standard state when ECU Mode Handling is used [doc:AUTOSAR_SWS_ECUStateManager, page:27, chunk:835]. The ECU Manager module arbitrates RUN and POST_RUN requests from software components [doc:AUTOSAR_SWS_ECUStateManager, page:13, chunk:780]. The mode port of the ECU State Manager module declares RUN as one of its modes, along with STARTUP, POST_RUN, SLEEP, and SHUTDOWN [doc:AUTOSAR_SWS_ECUStateManager, page:149, chunk:1270].\n",
      "================================================================================\n",
      "Q: How does AUTOSAR ensure a safe transition between ECU states?\n",
      "Based on the provided context, AUTOSAR ensures safe transitions between ECU states through a cooperative mechanism between the ECU State Manager (EcuM) and the BSW Mode Manager (BswM) [doc:AUTOSAR_SWS_ECUStateManager, page:92, chunk:1061].\n",
      "\n",
      "ECU states are generally implemented as AUTOSAR modes, and the BswM is responsible for monitoring and affecting the corresponding changes [doc:AUTOSAR_SWS_ECUStateManager, page:19, chunk:801]. While the EcuM arbitrates requests for mode changes from Software Components (SW-Cs), only the BswM can decide when a transition to a different mode can be made [doc:AUTOSAR_SWS_ECUStateManager, page:92, chunk:1061].\n",
      "\n",
      "For safety-critical operations, the ECU State Manager must run with a full trust level [doc:AUTOSAR_SWS_ECUStateManager, page:69, chunk:1003]. Additionally, methods to ensure a proper de-initialization phase must be upheld [doc:AUTOSAR_SWS_ECUStateManager, page:69, chunk:1003]. In case of an error where processing cannot continue, the ECU must be stopped, and the integrator can choose the modality, such as a reset, halt, or transition to a safe state [doc:AUTOSAR_SWS_ECUStateManager, page:124, chunk:1184]. The `EcuM_ErrorHook` can be invoked by the ECU Manager module in all phases to handle such errors [doc:AUTOSAR_SWS_ECUStateManager, page:124, chunk:1184].\n",
      "================================================================================\n",
      "Q: What is the difference between ASPICE SYS.2 and SWE.1 objectives?\n",
      "Based on the provided context, SYS.2 is named \"System Requirements Analysis\" and SWE.1 is named \"Software Requirements Analysis\" [doc:Automotive_SPICE_PAM_31_EN, page:122, chunk:695]. The Software Engineering process group (SWE), which includes SWE.1, addresses the management of software requirements that are derived from system requirements [doc:Automotive_SPICE_PAM_31_EN, page:12, chunk:73].\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "for q in queries:\n",
    "    res = answer_query_hybrid(q)\n",
    "    print(\"Q:\", q)\n",
    "    print(res[\"answer\"])\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce6d92d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
