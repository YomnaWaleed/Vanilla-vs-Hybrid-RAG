{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cf75414",
   "metadata": {},
   "source": [
    "### 1) Load index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4b893cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, json, os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f2f8e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abe05a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS\n",
    "emb = GoogleGenerativeAIEmbeddings(\n",
    "  model = \"models/embedding-001\", \n",
    "  google_api_key= GEMINI_API_KEY\n",
    ")\n",
    "faiss_store = FAISS.load_local(\"../artifacts/faiss_index\", emb, allow_dangerous_deserialization= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a803e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BM25 corpus\n",
    "with open(\"../artifacts/bm25_corpus.pkl\", \"rb\") as f:\n",
    "  bm25_data = pickle.load(f)\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "bm25 = BM25Okapi(bm25_data[\"corpus_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee48d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load chunks mapping\n",
    "chunks_map = {}\n",
    "with open(\"../artifacts/chunks.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "  for line in f:\n",
    "    chunk = json.loads(line)\n",
    "    chunks_map[chunk[\"chunk_id\"]] = chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e6dfcc",
   "metadata": {},
   "source": [
    "### 2) Retrieve Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a535cb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e28759cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(q):\n",
    "  return re.findall(r\"\\b\\w+\\b\", q.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e88b4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_bm25(query, k = 10):\n",
    "  scores = bm25.get_scores(tokenize(query))\n",
    "  idx = np.argsort(scores)[::-1][:k]\n",
    "  out = []\n",
    "  for i in idx:\n",
    "    out.append({\n",
    "      \"chunk_id\": i, \n",
    "      \"score\": float(scores[i]),\n",
    "      \"source\" : \"bm25\", \n",
    "      \"text\":chunks_map[i][\"text\"], \n",
    "      \"doc_id\": chunks_map[i][\"doc_id\"], \n",
    "      \"page\":chunks_map[i][\"page\"], \n",
    "      })\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7027ed4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_dense(query, k = 10):\n",
    "  docs_scores = faiss_store.similarity_search_with_score(query, k = k)\n",
    "  \n",
    "  out = []\n",
    "  for doc, dist in docs_scores:\n",
    "    cid = doc.metadata[\"chunk_id\"]\n",
    "    out.append({\n",
    "      \"chunk_id\": cid, \n",
    "      \"score\": float(-dist), \n",
    "      \"source\": \"dense\", \n",
    "      \"text\": doc.page_content, \n",
    "      \"doc_id\": chunks_map[cid][\"doc_id\"], \n",
    "      \"page\": doc.metadata.get(\"page\", None)\n",
    "    })\n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "573ba0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrf_fusion(candidates_lists, top_k = 8 , k_rff = 60):\n",
    "  # candidates_lists: [list_from_bm25, list_from_dense]\n",
    "  score_by_id = defaultdict(float)\n",
    "  seen_rank = defaultdict(dict)  # source -> chunk_id -> rank\n",
    "  \n",
    "  for cand_list in candidates_lists:\n",
    "    for rank, item in enumerate(cand_list):\n",
    "      cid = item[\"chunk_id\"]\n",
    "      score_by_id[cid] += 1.0 / (k_rff + rank +1)\n",
    "  \n",
    "  # retrieve best top_k \n",
    "  ranked = sorted(score_by_id.items(), key = lambda x: x[1], reverse=True)[:top_k]\n",
    "  \n",
    "  merged = []\n",
    "  pool = { (c[\"chunk_id\"], c[\"source\"]): c for lst in candidates_lists for c in lst }\n",
    "  \n",
    "  for cid, _ in ranked:\n",
    "    # find any item which chunk_id = cid \n",
    "    item = None\n",
    "    for lst in candidates_lists:\n",
    "      for c in lst:\n",
    "        if c[\"chunk_id\"] == cid:\n",
    "          item = c\n",
    "          break\n",
    "      if item:\n",
    "        break\n",
    "    merged.append(item)\n",
    "  return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabbce46",
   "metadata": {},
   "source": [
    "Reranker (Cross-Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc820fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPSHOP\\anaconda3\\envs\\slrag\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bd441b14b2466fa1a41655706b64f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LAPSHOP\\anaconda3\\envs\\slrag\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30d979b8fdf43729fa5ca89248c7e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcca80b5dae14a80bd752a32320591f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1072a49f06be47b3ae6ced135c65ee80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b25b932bf2e640258a5811bbeb0c821f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e336e5ad120477da1b4670bda743af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "import numpy as np\n",
    "\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "def rerank_with_crossencoder(query, candidates, top_k=6):\n",
    "    pairs = [(query, c[\"text\"]) for c in candidates]\n",
    "    scores = reranker.predict(pairs)  # bigger = better\n",
    "    order = np.argsort(scores)[::-1][:top_k]\n",
    "    reranked = [candidates[i] for i in order]\n",
    "    return reranked\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9136b9",
   "metadata": {},
   "source": [
    "### 3) Generation the response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e509a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e81ae0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "  api_key = GEMINI_API_KEY,\n",
    "  model = \"gemini-2.5-pro\", \n",
    "  temperature=0.2,\n",
    "  max_retries=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfc09816",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM = (\n",
    "\"You are an assistant answering automotive standards questions (ASPICE/AUTOSAR). \"\n",
    "\"Answer ONLY from the provided context. If the answer is not in the context, say 'Insufficient context'. \"\n",
    "\"After each factual sentence, add citations like [doc:{doc_id}, page:{page}, chunk:{chunk_id}].\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "315619ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context_snippets(evidences):\n",
    "  ctx = []\n",
    "  for e in evidences:\n",
    "        ctx.append(\n",
    "            f\"[doc:{e['doc_id']}, page:{e['page']}, chunk:{e['chunk_id']}]\\n{e['text']}\\n\"\n",
    "        )\n",
    "  return \"\\n---\\n\".join(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "531f3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query_hybrid(query, k_each=12, fuse_top=12, rerank_top=6):\n",
    "    bm25_hits = retrieve_bm25(query, k=k_each)\n",
    "    dense_hits = retrieve_dense(query, k=k_each)\n",
    "    fused = rrf_fusion([bm25_hits, dense_hits], top_k=fuse_top)\n",
    "    reranked = rerank_with_crossencoder(query, fused, top_k=rerank_top)\n",
    "\n",
    "    context_block = build_context_snippets(reranked)\n",
    "    prompt = (\n",
    "        f\"Question: {query}\\n\\n\"\n",
    "        f\"Context (use strictly):\\n{context_block}\\n\\n\"\n",
    "        \"Return a concise answer with citations as instructed.\"\n",
    "    )\n",
    "    msgs = [SystemMessage(content=SYSTEM), HumanMessage(content=prompt)]\n",
    "    resp = llm.invoke(msgs)\n",
    "    return {\"answer\": resp.content, \"evidence\": reranked}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a784a",
   "metadata": {},
   "source": [
    "### 4) practice on 5 queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5e7198",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
